{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hyperparameter Optimization on a NeRF.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lLDTVWKq7-ei"
      },
      "source": [
        "## Hyperparameter Optimization on a NeRF\n",
        "This is a simplied version of the method presented in *NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis*. The code is adapted from the [NeRF github repo](github.com/bmild/nerf).\n",
        "\n",
        "#### [See the results in a live dashboard →](https://app.wandb.ai/sweep/nerf/reports/NeRF-%E2%80%93-Representing-Scenes-as-Neural-Radiance-Fields-for-View-Synthesis--Vmlldzo3ODIzMA/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qwtF8nkxdHVf",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    %tensorflow_version 1.x\n",
        "\n",
        "import os, sys\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGHeM6OXhHlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb -qq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s8D-sZfikMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5mTxAwgrj4yn",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('tiny_nerf_data.npz'):\n",
        "    !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t2dgdCDi-m3T"
      },
      "source": [
        "# Load Input Images and Poses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj1lof2ej0FI",
        "colab": {}
      },
      "source": [
        "data = np.load('tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "H, W = images.shape[1:3]\n",
        "print(images.shape, poses.shape, focal)\n",
        "\n",
        "testimg, testpose = images[101], poses[101]\n",
        "images = images[:100,...,:3]\n",
        "poses = poses[:100]\n",
        "\n",
        "plt.imshow(testimg)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jxDt192E-v6i"
      },
      "source": [
        "# Optimize NeRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R1avtwVoAQTu",
        "colab": {}
      },
      "source": [
        "def posenc(x, embed_size):\n",
        "  rets = [x]\n",
        "  for i in range(embed_size):\n",
        "    for fn in [tf.sin, tf.cos]:\n",
        "      rets.append(fn(2.**i * x))\n",
        "  return tf.concat(rets, -1)\n",
        "\n",
        "embed_fn = posenc\n",
        "\n",
        "def get_rays(H, W, focal, c2w):\n",
        "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
        "    dirs = tf.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -tf.ones_like(i)], -1)\n",
        "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
        "    rays_o = tf.broadcast_to(c2w[:3,-1], tf.shape(rays_d))\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, embed_size, rand=False):\n",
        "\n",
        "    def batchify(fn, chunk=1024*32):\n",
        "        return lambda inputs : tf.concat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
        "    \n",
        "    # Compute 3D query points\n",
        "    z_vals = tf.linspace(near, far, N_samples) \n",
        "    if rand:\n",
        "      z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far-near)/N_samples\n",
        "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
        "    \n",
        "    # Run network\n",
        "    pts_flat = tf.reshape(pts, [-1,3])\n",
        "    pts_flat = embed_fn(pts_flat, embed_size)\n",
        "    raw = batchify(network_fn)(pts_flat)\n",
        "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
        "    \n",
        "    # Compute opacities and colors\n",
        "    sigma_a = tf.nn.relu(raw[...,3])\n",
        "    rgb = tf.math.sigmoid(raw[...,:3]) \n",
        "    \n",
        "    # Do volume rendering\n",
        "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[...,:1].shape)], -1) \n",
        "    alpha = 1.-tf.exp(-sigma_a * dists)  \n",
        "    weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
        "    \n",
        "    rgb_map = tf.reduce_sum(weights[...,None] * rgb, -2) \n",
        "    depth_map = tf.reduce_sum(weights * z_vals, -1) \n",
        "    acc_map = tf.reduce_sum(weights, -1)\n",
        "\n",
        "    return rgb_map, depth_map, acc_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O7OAd11TGz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from ipywidgets import interactive, widgets\n",
        "\n",
        "\n",
        "trans_t = lambda t : tf.convert_to_tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,t],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "rot_phi = lambda phi : tf.convert_to_tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,tf.cos(phi),-tf.sin(phi),0],\n",
        "    [0,tf.sin(phi), tf.cos(phi),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "rot_theta = lambda th : tf.convert_to_tensor([\n",
        "    [tf.cos(th),0,-tf.sin(th),0],\n",
        "    [0,1,0,0],\n",
        "    [tf.sin(th),0, tf.cos(th),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
        "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
        "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "def f(**kwargs):\n",
        "    c2w = pose_spherical(**kwargs)\n",
        "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "    img = np.clip(rgb,0,1)\n",
        "    \n",
        "    plt.figure(2, figsize=(20,6))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "sldr = lambda v, mi, ma: widgets.FloatSlider(\n",
        "    value=v,\n",
        "    min=mi,\n",
        "    max=ma,\n",
        "    step=.01,\n",
        ")\n",
        "\n",
        "names = [\n",
        "    ['theta', [100., 0., 360]],\n",
        "    ['phi', [-30., -90, 0]],\n",
        "    ['radius', [4., 3., 5.]],\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF8X2hb-keyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure the sweep – specify the parameters to search through, the search strategy, the optimization metric et all.\n",
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'embed_size': {\n",
        "            'values': [2, 6, 8, 10]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [10, 50, 100, 200, 500, 1000, 2000, 10000]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [5e-3, 7e-4, 5e-4, 3e-4, 5e-5, 5e-6]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'nadam', 'sgd', 'rmsprop']\n",
        "        },\n",
        "        'layers': {\n",
        "            'values': [4, 6, 8]\n",
        "        },\n",
        "        'dense_size': {\n",
        "            'values': [32, 128, 256]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5S686X-khl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a new sweep\n",
        "# Arguments:\n",
        "#     – sweep_config: the sweep config dictionary defined above\n",
        "#     – entity: Set the username for the sweep\n",
        "#     – project: Set the project name for the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"sweep\", project=\"nerf\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TSAyVcKAiyI"
      },
      "source": [
        "Here we optimize the model. We plot a rendered holdout view and its PSNR every 50 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6XurcHoCj0FQ",
        "colab": {}
      },
      "source": [
        "# The sweep calls this function with each set of hyperparameters\n",
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "    config_defaults = {\n",
        "        'epochs': 200,\n",
        "        'optimizer': 'nadam',\n",
        "        'learning_rate': 5e-4,\n",
        "        'embed_size': 6,\n",
        "        'layers': 8,\n",
        "        'dense_size': 256,\n",
        "        'N_samples': 64,\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults, name=\"epochs_50000\")\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "\n",
        "    relu = tf.keras.layers.ReLU()    \n",
        "    dense = lambda dense_size=config.dense_size, act=relu : tf.keras.layers.Dense(dense_size, activation=act)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(3 + 3*2*config.embed_size)) \n",
        "    outputs = inputs\n",
        "    for i in range(config.layers):\n",
        "        outputs = dense()(outputs)\n",
        "        if i%4==0 and i>0:\n",
        "            outputs = tf.concat([outputs, inputs], -1)\n",
        "    outputs = dense(4, act=None)(outputs)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    if config.optimizer=='sgd':\n",
        "      optimizer = tf.keras.optimizers.SGD(lr=config.learning_rate, decay=1e-5, momentum=0.9, nesterov=True)\n",
        "    elif config.optimizer=='rmsprop':\n",
        "      optimizer = tf.keras.optimizers.RMSprop(lr=config.learning_rate, decay=1e-5)\n",
        "    elif config.optimizer=='adam':\n",
        "      optimizer = tf.keras.optimizers.Adam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
        "    elif config.optimizer=='nadam':\n",
        "      optimizer = tf.keras.optimizers.Nadam(lr=config.learning_rate, beta_1=0.9, beta_2=0.999, clipnorm=1.0)\n",
        "\n",
        "    psnrs = []\n",
        "    iternums = []\n",
        "    i_plot = 1000\n",
        "\n",
        "    import time\n",
        "    t = time.time()\n",
        "    for i in range(config.epochs+1):\n",
        "        img_i = np.random.randint(images.shape[0])\n",
        "        target = images[img_i]\n",
        "        pose = poses[img_i]\n",
        "        rays_o, rays_d = get_rays(H, W, focal, pose)\n",
        "        with tf.GradientTape() as tape:\n",
        "            rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config.N_samples, embed_size = config.embed_size, rand=True)\n",
        "            loss = tf.reduce_mean(tf.square(rgb - target))\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "        if i%i_plot==0:\n",
        "            print(i, (time.time() - t) / i_plot, 'secs per iter')\n",
        "            t = time.time()\n",
        "            \n",
        "            # Render the holdout view for logging\n",
        "            rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
        "            rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=config.N_samples, embed_size = config.embed_size)\n",
        "            loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
        "            psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
        "            wandb.log({'psnr': psnr.numpy(), 'loss': loss.numpy()})\n",
        "            caption = \"Iteration: \"+str(i)\n",
        "            wandb.log({\"images\": [wandb.Image(rgb, caption=caption)]})\n",
        "\n",
        "            psnrs.append(psnr.numpy())\n",
        "            iternums.append(i)\n",
        "            \n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.subplot(121)\n",
        "            plt.imshow(rgb)\n",
        "            plt.title(f'Iteration: {i}')\n",
        "            plt.subplot(122)\n",
        "            plt.plot(iternums, psnrs)\n",
        "            plt.title('PSNR')\n",
        "            plt.show()\n",
        "            print('psnr', psnr.numpy())\n",
        "            print('loss', loss.numpy())\n",
        "\n",
        "    print('Done')\n",
        "    frames = []\n",
        "    for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n",
        "        c2w = pose_spherical(th, -30., 4.)\n",
        "        rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., embed_size=config.embed_size, N_samples=64)\n",
        "        frames.append((255*np.clip(rgb,0,1)).astype(np.uint8))\n",
        "\n",
        "    import imageio\n",
        "    f = 'video.mp4'\n",
        "    imageio.mimwrite(f, frames, fps=30, quality=7)\n",
        "\n",
        "    wandb.log({\"video_epochs_500\": wandb.Video(f, fps=30, format=\"mp4\")})\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzb4-ZT2ZtIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}